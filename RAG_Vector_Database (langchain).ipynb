{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMA5y85xOgY5jSWDHArp5cL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["By [Yulandy Chiu](https://www.youtube.com/@YulandySpace)\n","\n","Aided with Gemini/Claude/ChatGPT and modified by Yulandy Chiu\n","\n","Version: 2024/12/27\n","\n","Video: [RAG implementation using LangChain and Vector Database](https://youtu.be/1qB-opvJxnU)\n","\n"," This code is licensed under the Creative Commons Attribution-NonCommercial 4.0\n"," International License (CC BY-NC 4.0). You are free to use, modify, and share this code for non-commercial purposes, provided you give appropriate credit. For more details, see the LICENSE file or visit: https://creativecommons.org/licenses/by-nc/4.0/\n"," © [2024] [Yulandy Chiu](https://www.youtube.com/@YulandySpace)"],"metadata":{"id":"QL2VeDjUONnH"}},{"cell_type":"code","source":["# Step 1: Install required packages\n","!pip install google-generativeai langchain-google-genai faiss-cpu sentence-transformers pypdf\n","!pip install langchain-community\n","!pip install unstructured python-docx\n","!pip install python-magic\n","!pip install libmagic\n","!pip install -U langchain-huggingface\n","import IPython\n","IPython.display.clear_output()\n","print(\"All packages installed!\")"],"metadata":{"id":"_X4WaU0dAHXY"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"id":"6PWU_rI0kt00","executionInfo":{"status":"ok","timestamp":1735343937579,"user_tz":-480,"elapsed":8803,"user":{"displayName":"Yulandy","userId":"07760009329158217518"}}},"outputs":[],"source":["# Step 2: install libraries and define functions\n","import os\n","from typing import List, Dict\n","import glob\n","import google.generativeai as genai\n","from langchain_google_genai import GoogleGenerativeAI\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain_community.document_loaders import (\n","    PyPDFLoader,\n","    UnstructuredWordDocumentLoader,\n","    TextLoader,\n","    CSVLoader\n",")\n","from langchain_community.vectorstores import FAISS\n","from langchain_huggingface import HuggingFaceEmbeddings\n","from langchain.chains import ConversationalRetrievalChain\n","from google.colab import userdata\n","\n","class DocumentProcessor:\n","    def __init__(self):\n","        \"\"\"\n","        Initialize Document Processor\n","        \"\"\"\n","        try:\n","            # Get API key from Colab Secrets\n","            api_key = userdata.get('Gemini_API_Key')\n","            if not api_key:\n","                raise ValueError(\"Cannot get Gemini_API_Key from Colab Secrets\")\n","\n","            # Configure Google API\n","            genai.configure(api_key=api_key)\n","\n","            # Initialize Gemini model\n","            self.llm = GoogleGenerativeAI(\n","                model=\"gemini-1.5-flash\",\n","                google_api_key=api_key,\n","                temperature=0.3\n","            )\n","\n","            # Initialize embeddings model\n","            self.embeddings = HuggingFaceEmbeddings(\n","                model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n","                model_kwargs={'device': 'cpu'}\n","            )\n","\n","            self.vector_store = None\n","            self.chain = None\n","            self.processed_files = []\n","\n","            print(\"System initialized successfully!\")\n","\n","        except Exception as e:\n","            print(f\"Initialization failed: {str(e)}\")\n","            raise\n","\n","    def scan_directory(self, directory_path: str) -> Dict[str, List[str]]:\n","        \"\"\"\n","        Scan directory for supported file types\n","        Args:\n","            directory_path: Directory path containing documents\n","        Returns:\n","            Dict[str, List[str]]: Dictionary of file paths grouped by type\n","        \"\"\"\n","        try:\n","            if not os.path.exists(directory_path):\n","                raise ValueError(f\"Directory does not exist: {directory_path}\")\n","\n","            file_types = {\n","                'pdf': '*.pdf',\n","                'docx': '*.docx',\n","                'doc': '*.doc',\n","                'txt': '*.txt',\n","                'csv': '*.csv'\n","            }\n","\n","            files_by_type = {type_: [] for type_ in file_types}\n","\n","            for root, _, _ in os.walk(directory_path):\n","                for file_type, pattern in file_types.items():\n","                    file_pattern = os.path.join(root, pattern)\n","                    found_files = glob.glob(file_pattern)\n","                    files_by_type[file_type].extend(found_files)\n","\n","            total_files = sum(len(files) for files in files_by_type.values())\n","            if total_files == 0:\n","                print(f\"No supported files found in {directory_path}\")\n","                return {}\n","\n","            print(f\"Found {total_files} files:\")\n","            for file_type, files in files_by_type.items():\n","                if files:\n","                    print(f\"\\n{file_type.upper()} files ({len(files)}):\")\n","                    for file in files:\n","                        print(f\"- {os.path.basename(file)}\")\n","\n","            return files_by_type\n","\n","        except Exception as e:\n","            print(f\"Error scanning directory: {str(e)}\")\n","            return {}\n","\n","    def load_document(self, file_path: str) -> List:\n","        \"\"\"\n","        Load document based on file type\n","        Args:\n","            file_path: Path to the document\n","        Returns:\n","            List: List of document objects\n","        \"\"\"\n","        file_extension = os.path.splitext(file_path)[1].lower()\n","\n","        try:\n","            if file_extension in ['.doc', '.docx']:\n","                loader = UnstructuredWordDocumentLoader(file_path)\n","            elif file_extension == '.pdf':\n","                loader = PyPDFLoader(file_path)\n","            elif file_extension == '.txt':\n","                loader = TextLoader(file_path)\n","            elif file_extension == '.csv':\n","                loader = CSVLoader(file_path)\n","            else:\n","                raise ValueError(f\"Unsupported file type: {file_extension}\")\n","\n","            documents = loader.load()\n","\n","            # Add source metadata\n","            for doc in documents:\n","                doc.metadata[\"source\"] = os.path.basename(file_path)\n","                if \"page\" not in doc.metadata:\n","                    doc.metadata[\"page\"] = 1\n","\n","            return documents\n","\n","        except Exception as e:\n","            print(f\"Error loading {os.path.basename(file_path)}: {str(e)}\")\n","            return []\n","\n","    def process_documents(self, files_by_type: Dict[str, List[str]]) -> bool:\n","        \"\"\"\n","        Process multiple documents and create unified vector database\n","        Args:\n","            files_by_type: Dictionary of file paths grouped by type\n","        Returns:\n","            bool: Whether processing was successful\n","        \"\"\"\n","        try:\n","            all_texts = []\n","            self.processed_files = []\n","\n","            for file_type, files in files_by_type.items():\n","                for file_path in files:\n","                    try:\n","                        print(f\"\\nProcessing {file_type.upper()}: {os.path.basename(file_path)}\")\n","\n","                        documents = self.load_document(file_path)\n","                        if not documents:\n","                            continue\n","\n","                        text_splitter = RecursiveCharacterTextSplitter(\n","                            chunk_size=1000,\n","                            chunk_overlap=200,\n","                            length_function=len\n","                        )\n","                        texts = text_splitter.split_documents(documents)\n","                        all_texts.extend(texts)\n","                        self.processed_files.append(os.path.basename(file_path))\n","                        print(f\"Successfully processed {len(texts)} text segments\")\n","\n","                    except Exception as e:\n","                        print(f\"Error processing {os.path.basename(file_path)}: {str(e)}\")\n","                        continue\n","\n","            if not all_texts:\n","                print(\"No documents were successfully processed\")\n","                return False\n","\n","            print(f\"\\nProcessed {len(self.processed_files)} files, {len(all_texts)} text segments\")\n","\n","            self.vector_store = FAISS.from_documents(\n","                documents=all_texts,\n","                embedding=self.embeddings\n","            )\n","                # limit the response to the retrieved content\n","            self.chain = ConversationalRetrievalChain.from_llm(\n","                llm=self.llm,\n","                retriever=self.vector_store.as_retriever(\n","                    search_kwargs={\"k\": 3}\n","                ),\n","                return_source_documents=True\n","            )\n","\n","            print(\"\\nVector database created successfully!\")\n","            print(\"Processed files:\")\n","            for file in self.processed_files:\n","                print(f\"- {file}\")\n","\n","            return True\n","\n","        except Exception as e:\n","            print(f\"Error processing documents: {str(e)}\")\n","            return False\n","\n","    def ask_question(self, question: str) -> Dict:\n","        \"\"\"\n","        Ask questions about the documents\n","        Args:\n","            question: Question content\n","        Returns:\n","            Dict: Dictionary containing answer and source documents\n","        \"\"\"\n","        try:\n","            if not self.chain:\n","                raise ValueError(\"Please process documents first!\")\n","\n","            print(\"Thinking about the question...\")\n","            response = self.chain({\"question\": question, \"chat_history\": []})\n","\n","            return {\n","                \"answer\": response[\"answer\"],\n","                \"sources\": [\n","                    {\n","                        \"file\": doc.metadata[\"source\"],\n","                        \"page\": doc.metadata[\"page\"],\n","                        \"content\": doc.page_content[:200] + \"...\"\n","                    }\n","                    for doc in response[\"source_documents\"]\n","                ]\n","            }\n","\n","        except Exception as e:\n","            print(f\"Error answering question: {str(e)}\")\n","            return {\"error\": str(e)}\n"]},{"cell_type":"code","source":["# Step 3: Create a folder for uploading and storing files manually\n","\n","document_directory = \"/content/source\"\n","if not os.path.exists(document_directory):\n","    os.makedirs(document_directory)"],"metadata":{"id":"PKgew0efkxBF","executionInfo":{"status":"ok","timestamp":1735343945224,"user_tz":-480,"elapsed":351,"user":{"displayName":"Yulandy","userId":"07760009329158217518"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Step 4: Scan the directory for files, process documents, and create a vector database\n","processor = DocumentProcessor()\n","\n","files_by_type = processor.scan_directory(document_directory)\n","if not files_by_type or not processor.process_documents(files_by_type):\n","  exit(\"File processing failed.\")"],"metadata":{"id":"H3D7X2BZt0Ib"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 5: Query the vector database with a question and retrieve the answer with referecnes\n","response = processor.ask_question(\"Yulandy有哪些朋友?\")\n","print(\"\\nAnswer:\", response[\"answer\"])\n","print(\"\\nReference Sources:\")\n","for source in response[\"sources\"]:\n","    print(f\"- File: {source['file']}, Page {source['page']}\")\n","    print(f\"  Content: {source['content']}\")\n"],"metadata":{"id":"kNYpR5IbmWon"},"execution_count":null,"outputs":[]}]}