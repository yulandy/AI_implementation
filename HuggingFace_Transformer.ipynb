{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "By [Yulandy Chiu](https://www.youtube.com/@YulandySpace)\n",
        "\n",
        "Aided with Gemini/Claude/ChatGPT and modified by Yulandy Chiu\n",
        "\n",
        "Version: 2025/02\n",
        "\n",
        "Videos:\n",
        "* [Hugging Face 快速入門：三大核心功能 Hub + Spaces + Transformers 庫 | HF API Key & Google Colab Secret 設定](https://youtu.be/e0xBXA3hUpQ)\n",
        "\n",
        "YouTube: [Yulandy Chiu的AI觀測站](https://www.youtube.com/@YulandySpace)\n",
        "\n",
        "Facebook: [Yulandy Chiu的AI資訊站](https://www.facebook.com/yulandychiu)\n",
        "\n",
        " This code is licensed under the Creative Commons Attribution-NonCommercial 4.0\n",
        " International License (CC BY-NC 4.0). You are free to use, modify, and share this code for non-commercial purposes, provided you give appropriate credit. For more details, see the LICENSE file or visit: https://creativecommons.org/licenses/by-nc/4.0/\n",
        " © [2025] Yulandy Chiu\n"
      ],
      "metadata": {
        "id": "yoIZVfbY4MrB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "測試Hugging Face API Key設定"
      ],
      "metadata": {
        "id": "TDmdTe5WmDWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "Python_practice=userdata.get('Python_practice')\n",
        "print(Python_practice)"
      ],
      "metadata": {
        "id": "Ih-M7mySR0n7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BertForSequenceClassification (transformer) 模型直接使用"
      ],
      "metadata": {
        "id": "zYlRkZ-vFXFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn.functional as F\n",
        "\n",
        "## BertForSequenceClassification以Transformer架構為基礎：\n",
        "# 專門優化用於分類任務\n",
        "# 只使用編碼器部分\n",
        "# 增加了一個分類頭\n",
        "\n",
        "# BERT 全名是 \"Bidirectional Encoder Representations from Transformers\"，由 Google AI Language\n",
        "\n",
        "# 準備數據\n",
        "texts = [\n",
        "    \"這部電影真的很好看\",\n",
        "    \"服務態度很差\",\n",
        "    \"價格合理，品質優良\",\n",
        "    \"完全不推薦這家餐廳\",\n",
        "    \"這家餐廳的菜品非常美味\",\n",
        "    \"電影情節很無聊\",\n",
        "    \"服務人員態度親切\",\n",
        "    \"價格太貴了，不值得\",\n",
        "    \"環境非常舒適\",\n",
        "    \"等候時間太長了\"\n",
        "]\n",
        "labels = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]  # 1: 正面, 0: 負面\n",
        "\n",
        "# 自定義數據集\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=64):\n",
        "        self.encodings = tokenizer(texts,\n",
        "                                 truncation=True,\n",
        "                                 padding=True,\n",
        "                                 max_length=max_length,\n",
        "                                 return_tensors='pt')\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "def train_model(model, train_loader, val_loader, device, epochs=3):\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "    best_accuracy = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # 訓練階段\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids,\n",
        "                          attention_mask=attention_mask,\n",
        "                          labels=labels)\n",
        "\n",
        "            loss = outputs.loss\n",
        "            train_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # 驗證階段\n",
        "        model.eval()\n",
        "        val_accuracy = 0\n",
        "        val_count = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "\n",
        "                outputs = model(input_ids=input_ids,\n",
        "                              attention_mask=attention_mask)\n",
        "\n",
        "                predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "                val_accuracy += (predictions == labels).sum().item()\n",
        "                val_count += labels.size(0)\n",
        "\n",
        "        val_accuracy = val_accuracy / val_count\n",
        "        print(f'Epoch {epoch + 1}:')\n",
        "        print(f'Average training loss: {train_loss / len(train_loader):.4f}')\n",
        "        print(f'Validation accuracy: {val_accuracy:.4f}\\n')\n",
        "\n",
        "        if val_accuracy > best_accuracy:\n",
        "            best_accuracy = val_accuracy\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "\n",
        "def predict_sentiment(text, model, tokenizer, device):\n",
        "    model.eval()\n",
        "    encoding = tokenizer(text,\n",
        "                        truncation=True,\n",
        "                        padding=True,\n",
        "                        max_length=64,\n",
        "                        return_tensors='pt')\n",
        "\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids,\n",
        "                       attention_mask=attention_mask)\n",
        "        probabilities = F.softmax(outputs.logits, dim=-1)\n",
        "        prediction = torch.argmax(probabilities, dim=-1)\n",
        "        confidence = torch.max(probabilities).item()\n",
        "\n",
        "    sentiment = \"正面\" if prediction.item() == 1 else \"負面\"\n",
        "    return sentiment, confidence\n",
        "\n",
        "\n",
        "# 設置設備\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"使用設備: {device}\")\n",
        "\n",
        "# 加載tokenizer和模型\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-chinese',\n",
        "    num_labels=2\n",
        ").to(device)\n",
        "\n",
        "# 分割數據集\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "        texts, labels, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "# 創建數據加載器\n",
        "train_dataset = SentimentDataset(train_texts, train_labels, tokenizer)\n",
        "val_dataset = SentimentDataset(val_texts, val_labels, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4)\n",
        "\n",
        "\n",
        "# 測試一些新的文本\n",
        "test_texts = [\n",
        "        \"這部電影真的很好看\",\n",
        "        \"服務態度很差\",\n",
        "        \"價格合理，品質優良\",\n",
        "        \"完全不推薦這家餐廳\"\n",
        "    ]\n",
        "\n",
        "\n",
        "print(\"Pretrained model測試結果:\")\n",
        "for text in test_texts:\n",
        "     sentiment, confidence = predict_sentiment(text, model, tokenizer, device)\n",
        "     print(f\"\\n文本: {text}\")\n",
        "     print(f\"預測情感: {sentiment}\")\n",
        "     print(f\"信心度: {confidence*100:.2f}%\")"
      ],
      "metadata": {
        "id": "X4s-vH9uDZrT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}